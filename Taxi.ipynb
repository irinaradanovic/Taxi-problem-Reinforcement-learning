{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf6ccfd",
   "metadata": {},
   "source": [
    "# Extended Taxi problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d30344",
   "metadata": {},
   "source": [
    "In this project we will implement the basic Taxi problem  with added gas station and slippery fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90e4c5",
   "metadata": {},
   "source": [
    "The taxi moves along the 5x5 grid. \n",
    "Its task consists of three phases: \n",
    "1. Finding the passenger: going to one of the 4 stations (R, G, Y, B) where the passenger is.\n",
    "2. Executing the PICKUP action\n",
    "3. Going towards the destination and executing a DROPOFF action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e3e08",
   "metadata": {},
   "source": [
    "Additions to the problem:\n",
    "1. Resource Management (Fuel): Each movement consumes 1 unit of fuel. If the fuel reaches 0, the taxi stops and the episode ends in failure. The agent must learn when to turn to the gas station.\n",
    "2. Stochasticity (Ice): On certain fields (ice patches), the taxi can slip. This means that the agent gives the command \"Go North\", but ends up e.g. \"East\". This teaches the agent to avoid those fields or plan for failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5260d0",
   "metadata": {},
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830cbca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89645e7a",
   "metadata": {},
   "source": [
    "# Custom Taxi Environment With Fuel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e65e236",
   "metadata": {},
   "source": [
    "**Our custom environment will inherit from gymnasium.Env that defines the structure all environments must follow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344318f3",
   "metadata": {},
   "source": [
    "Fuel: max 16 units (0-15)\n",
    "\n",
    "Passenger location: 0=R, 1=G, 2=Y, 3=B, 4=in taxi.\n",
    "\n",
    "Destination: 0=R, 1=G, 2=Y, 3=B\n",
    "\n",
    "State is defined by **[row(taxi position), column(taxi position), passenger_location, destination, fuel]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fedf3f",
   "metadata": {},
   "source": [
    "Actions:\n",
    "\n",
    "    0-> N\n",
    "    1-> S\n",
    "    2-> E\n",
    "    3-> W\n",
    "    4->PICKUP\n",
    "    5->DROPOFF\n",
    "    6->REFUEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiFuelEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_size = 5\n",
    "        self.fuel_limit = 15\n",
    "\n",
    "        # MultiDiscrete - Supports multiple discrete values with multiple axes, used for controller actions\n",
    "        self.observation_space = spaces.MultiDiscrete([5, 5, 5, 4, 16]) #5 rows and columns, 5 passenger locations, 4 destinations, 16 units of fuel\n",
    "\n",
    "        self.action_space = spaces.Discrete(7)\n",
    "        self.locs = [(0,0), (0,4), (4,0), (4,3)] # R, G, Y, B\n",
    "        self.gas_station = (2,2) \n",
    "        self.ice_patches - [(1, 1), (1, 3), (3, 2)] #slippery sections\n",
    "\n",
    "    \n",
    "    def reset(self, seed = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.taxi_pos = [np.random.randint(0,5), np.random.randint(0,5)] #setting the taxi on a random position in the board\n",
    "        self.pass_idx = np.random.randint(0,4)  #the passenger is at one of these positions in the beginning: R, Y, B, G\n",
    "        self.dest_idx = np.random.randint(0,4)  \n",
    "        self.fuel = self.fuel_limit  #starting with maximum fuel\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        #convert internal state to observation format\n",
    "        return (self.taxi_pos[0], self.taxi_pos[1], self.pass_idx, self.dest_idx, self.fuel)\n",
    "    \n",
    "    def step(self, action):\n",
    "        return super().step(action)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f5e3f",
   "metadata": {},
   "source": [
    "# Q Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, state_space, action_space):\n",
    "        ''' \n",
    "            state_space - dimensions of the discrete state space\n",
    "            action_space - possible actions\n",
    "            alpha - learning rate\n",
    "            gamma - discount factor\n",
    "            epsilon - probability of a random action\n",
    "            Q - Q table\n",
    "        '''\n",
    "\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.99\n",
    "        # When epsilon = 1, the epsilon-greedy policy chooses actions completely randomly, so it is equivalent to a random policy.\n",
    "        self.epsilon = 1.0   \n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.actions = action_space # [-10, +10]\n",
    "\n",
    "        # Q table ->  Q[x_i][xdot_i][θ_i][θdot_i][action]\n",
    "        self.Q = np.zeros(state_space + (len(action_space),))\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        '''\n",
    "            Actions are selected using an epsilon-greedy policy,\n",
    "            so that exploration and exploitation are balanced.\n",
    "        '''\n",
    "        if np.random.rand() < self.epsilon:   \n",
    "            # Exploration - choose a random action\n",
    "            return np.random.randint(len(self.actions))\n",
    "        # Exploitation with probability 1-epsilon, the agent is using his current knowledge to maximize rewards\n",
    "        return np.argmax(self.Q[state]) \n",
    "    \n",
    "    def update(self, s, a, r, s_next):\n",
    "        '''Q-values are updated using the Bellman optimality equation\n",
    "            s - state, tuple\n",
    "            a - action, 0 or 1\n",
    "            r - reward\n",
    "            s_next - next state\n",
    "        '''\n",
    "        self.Q[s + (a,)] += self.alpha * (r + self.gamma * np.max(self.Q[s_next]) - self.Q[s + (a,)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
